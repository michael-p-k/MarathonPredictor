{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f94ca57c-a81f-4cf8-84a7-1669dd1c050a",
   "metadata": {},
   "source": [
    "# Marathon Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fddfdd6-0871-4556-a8cf-710c4d1f504c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d7ab83a-7575-4f7c-9ce8-d26a20f2f401",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import ssl\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Ignore SSL certificate errors. Need to also have 'context=ctx' as urlopen parameter.\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc1c8bd-be25-4fc9-b41c-bfb3131ed329",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Pull Race Info and Results\n",
    "\n",
    "__Choose one__ of the following to collect marathon results from MarathonGuide.com:\n",
    "\n",
    "- Option A: Save each marathon to an html file, then this code block parses info from all the pages.\n",
    "- Option B: Use Selenium webdriver to navigate through pages and scrape info (skips deciles to reduce time and requests).\n",
    "- Option C: Use Selenium webdriver to navigate through pages and scrape info (finds deciles for additional data).\n",
    "\n",
    "Note: __Option A is preferred__ since it does not hit the site with automated requests. Options B and C use automated requests using Selenium web driver that tend to get blocked eventually, despite added sleep/wait times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c233ce-12c9-4735-9081-2c60034da59b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Option A: Parse marathon results from locally saved html files ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96828925-f8c7-40ba-9376-57052d76cc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "<html>\n",
      "<head><script type=\"text/javascript\">/* <![CDATA[ */_cf_loadingtexthtml=\"<img alt=' ' src='/CFIDE/scripts/ajax/resources/cf/images/loading.gif'/>\";\n",
      "_cf_contextpath=\"\";\n",
      "_cf_ajaxscriptsrc=\"/CFIDE/scripts/ajax\";\n",
      "_cf_jsonprefix='//';\n",
      "_cf_websocket_port=8577;\n",
      "_cf_flash_policy_port=1243;\n",
      "_cf_cli\n"
     ]
    }
   ],
   "source": [
    "# Save each page of MarathonGuide.com you want to parse as an html file in a subfolder named results_html\n",
    "# Test if results are readable\n",
    "html = open(\"results_html/Twin Cities Marathon Race Results 2013.html\", \"r\")\n",
    "print(html.read()[0:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db775350-abd9-4a67-88bc-7b53cfba68ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scrape race results for all race events\n",
    "# HTML saved locally for each race/year because my selenium scraper got blocked by the site\n",
    "\n",
    "event_df = pd.DataFrame({'Event ID':[]}).set_index('Event ID')\n",
    "\n",
    "for file in os.listdir(\"results_html\"):\n",
    "    if file.endswith(\".html\"):\n",
    "        page_source = open(os.path.join(\"results_html\", file), \"r\").read()\n",
    "        \n",
    "        try:\n",
    "            event_id = re.findall(r'MIDD=(\\d+)', page_source)[0]\n",
    "            # Find the event name, date, city, state, number of finishers, etc.\n",
    "            event_df.loc[event_id,'Event Name'] = re.findall(\n",
    "                r'<b>(.+)</b> - Results', page_source)[0]\n",
    "            event_df.loc[event_id,'Date'] = re.findall(\n",
    "                r'[A-Z][a-z]+ \\d{1,2}, \\d{4}', page_source)[1]\n",
    "            event_df.loc[event_id,['City','State']] = re.findall(\n",
    "                # r'([\\w\\-\\.\\/ ]+), ([A-Z]{2})', page_source)[0] # Doesn't work on \"Phoenix, Scottsdale & Tempe\"\n",
    "                r'([^\\>]+), ([A-Z]{2})', page_source)[0]\n",
    "            event_df.loc[event_id,['Finishers','Males','Females']] = re.findall(\n",
    "                r'Finishers: (\\d+), Males - (\\d+) , Females - (\\d+)', page_source)[0]\n",
    "            event_df.loc[event_id,['Male Win','Female Win']] = re.findall(\n",
    "                r'Male Winner: ([\\d:]+) \\| Female Winner: ([\\d:]+)', page_source)[0]\n",
    "            event_df.loc[event_id,['Average Time','Time STD']] = re.findall(\n",
    "                r'Average Finish Time: ([\\d:]+) \\| STD: ([\\d:]+)', page_source)[0]\n",
    "\n",
    "        # In case the event page is missing some data, retrieve in opposite order to get any more data fields you can\n",
    "        except:\n",
    "            try:\n",
    "                event_id = re.findall(r'MIDD=(\\d+)', page_source)[0]\n",
    "                # Find the event name, date, city, state, number of finishers, etc.\n",
    "                event_df.loc[event_id,['Average Time','Time STD']] = re.findall(\n",
    "                    r'Average Finish Time: ([\\d:]+) \\| STD: ([\\d:]+)', page_source)[0]\n",
    "                event_df.loc[event_id,['Male Win','Female Win']] = re.findall(\n",
    "                    r'Male Winner: ([\\d:]+) \\| Female Winner: ([\\d:]+)', page_source)[0]\n",
    "                event_df.loc[event_id,['Finishers','Males','Females']] = re.findall(\n",
    "                    r'Finishers: (\\d+), Males - (\\d+) , Females - (\\d+)', page_source)[0]\n",
    "                event_df.loc[event_id,['City','State']] = re.findall(\n",
    "                    r'([^\\>]+), ([A-Z]{2})', page_source)[0]\n",
    "                event_df.loc[event_id,'Date'] = re.findall(\n",
    "                    r'[A-Z][a-z]+ \\d{1,2}, \\d{4}', page_source)[2]\n",
    "                event_df.loc[event_id,'Event Name'] = re.findall(\n",
    "                    r'<b>(.+)</b> - Results', page_source)[0]\n",
    "            except:\n",
    "                # print(event_df.loc[event_id],'\\n') # Can print the missing rows if desired\n",
    "                pass\n",
    "\n",
    "# Fill in gaps I know about\n",
    "event_df.loc['57001007',['Male Win','Female Win']] = ('2:21:06','2:51:02') # 2000 St George\n",
    "event_df.loc['2050116',['Male Win','Female Win']] = ('2:14:50','2:32:27') # 2005 Houston\n",
    "event_df.loc['67041010',['Male Win','Female Win']] = ('2:06:16','2:23:45') # 2004 Chicago\n",
    "event_df.loc['41061029',['Male Win','Female Win']] = ('2:21:21','3:00:23') # 2006 Marine Corps\n",
    "event_df.loc['15060417',['Male Win','Female Win']] = ('2:07:14','2:23:38') # 2006 Boston\n",
    "event_df.loc['474041128','Time STD'] = '1:01:19' # 2004 Seattle\n",
    "# Clean up names\n",
    "event_df.replace({'Chronicle Marathon':'San Francisco Marathon',\n",
    "                  'The San Francisco Marathon':'San Francisco Marathon',\n",
    "                  'PORTLANDATHON':'Portland Marathon',\n",
    "                  'Portland Oregon Marathon':'Portland Marathon',\n",
    "                  'City of Los Angeles Marathon (L.A. Marathon)':'L.A. Marathon',\n",
    "                  'Ottawa Marathon (National Capital Race Weekend)':'Ottawa Marathon',\n",
    "                  \"Rock 'n' Roll Marathon\":\"Rock 'n' Roll San Diego Marathon\"\n",
    "                 }, inplace=True)\n",
    "# Reduce compound cities to one city (the first in the list):\n",
    "# Phoenix, Scottsdale & Tempe -> Phoenix\n",
    "# Minneapolis/St. Paul -> Minneapolis\n",
    "event_df['City'] = event_df.apply(lambda x: re.split('/|,',x.City)[0], axis=1)\n",
    "\n",
    "# Save the event results to a csv\n",
    "event_df = (event_df.sort_values(by=['City','Event ID'],ascending=False)\n",
    "            .reset_index())\n",
    "directory = 'results_csvs'\n",
    "if not os.path.exists(directory):\n",
    "    print(f'Directory created: {directory}')\n",
    "    os.mkdir(directory)\n",
    "\n",
    "now = datetime.today().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "event_df.to_csv(f'{directory}/all_events_{now}.csv',index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2278f907-5b52-48c4-8ce2-da2fe87ba9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Event ID</th>\n",
       "      <th>Event Name</th>\n",
       "      <th>Date</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Finishers</th>\n",
       "      <th>Males</th>\n",
       "      <th>Females</th>\n",
       "      <th>Male Win</th>\n",
       "      <th>Female Win</th>\n",
       "      <th>Average Time</th>\n",
       "      <th>Time STD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>479</td>\n",
       "      <td>479</td>\n",
       "      <td>479</td>\n",
       "      <td>479</td>\n",
       "      <td>479</td>\n",
       "      <td>479</td>\n",
       "      <td>479</td>\n",
       "      <td>479</td>\n",
       "      <td>479</td>\n",
       "      <td>479</td>\n",
       "      <td>479</td>\n",
       "      <td>479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>479</td>\n",
       "      <td>25</td>\n",
       "      <td>429</td>\n",
       "      <td>23</td>\n",
       "      <td>17</td>\n",
       "      <td>470</td>\n",
       "      <td>460</td>\n",
       "      <td>461</td>\n",
       "      <td>422</td>\n",
       "      <td>430</td>\n",
       "      <td>461</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>41191027</td>\n",
       "      <td>Boston Marathon</td>\n",
       "      <td>October 7, 2007</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>CA</td>\n",
       "      <td>3218</td>\n",
       "      <td>1755</td>\n",
       "      <td>1597</td>\n",
       "      <td>2:11:56</td>\n",
       "      <td>2:42:15</td>\n",
       "      <td>4:32:02</td>\n",
       "      <td>0:47:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>105</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Event ID       Event Name             Date     City State Finishers  \\\n",
       "count        479              479              479      479   479       479   \n",
       "unique       479               25              429       23    17       470   \n",
       "top     41191027  Boston Marathon  October 7, 2007  Seattle    CA      3218   \n",
       "freq           1               22                3       32   105         2   \n",
       "\n",
       "       Males Females Male Win Female Win Average Time Time STD  \n",
       "count    479     479      479        479          479      479  \n",
       "unique   460     461      422        430          461      421  \n",
       "top     1755    1597  2:11:56    2:42:15      4:32:02  0:47:39  \n",
       "freq       2       2        5          4            2        4  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the table of marathon results we just made\n",
    "event_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6a53f2-e9c1-44d8-bb3e-bcdf1e9af5dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Option B: Parse marathon results with BS4 (skip decile times to reduce page requests)\n",
    "\n",
    "Warning: you should limit your data scraping using this method due to the high number of automated requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c72e8db-c54c-42f6-9e02-736d30e513bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary where we'll add a df for each event\n",
    "events_dfs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c89a750-63b5-481d-8145-c8d0efb362d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of initial race IDs for scraping MarathonGuide.com \n",
    "# Can find these on their website by clicking a particular race and year and looking at the end of the url.\n",
    "# Each event only needs race ID for one year added, since the race IDs for other years are automatically parsed.\n",
    "init_ids = {'Boston':'15000417',\n",
    "    'Chicago':'67001022',\n",
    "    'New York City':'472001105',\n",
    "    'LA':'9000305',\n",
    "    'Marine Corps':'41001022',\n",
    "    'Honolulu':'480001210',\n",
    "    'Disney World':'481000108',\n",
    "    'Rock n Roll San Diego':'27000604',\n",
    "    'Philadelphia':'479001119',\n",
    "    'Twin Cities':'58001008',\n",
    "    'Portland':'38001001',\n",
    "    'Houston':'2000116',\n",
    "    'California International':'687001203',\n",
    "    'St George':'57001007',\n",
    "    'Grandmas':'42000617',\n",
    "    'San Francisco':'521050731',\n",
    "    'Rock N Roll Arizona':''\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc12514-7b65-496c-8c29-15c1158eb56e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scrape race results for all race events.\n",
    "# Increase time.sleep(x) in appropriate spot(s) if needed.\n",
    "\n",
    "for event, init_id in init_ids.items():\n",
    "    # Create driver and point to initial url\n",
    "    init_url = f'http://www.marathonguide.com/results/browse.cfm?MIDD={init_id}'\n",
    "    driver = webdriver.Safari()\n",
    "    driver.get(init_url)\n",
    "    sleeptime = np.random.uniform(26, 29)\n",
    "    time.sleep(sleeptime)\n",
    "    \n",
    "    # Find all the event IDs in the 'a' tag hyperlinks. Each corresponds to a year of the event.\n",
    "    event_ids = sorted(list(set(re.findall(r'MIDD=(\\d+)', driver.page_source))), reverse=True)\n",
    "    \n",
    "    # Create df for this event. First column is event ID.\n",
    "    event_df = (pd.DataFrame({'Event ID':event_ids})\n",
    "                .set_index('Event ID'))\n",
    "    \n",
    "    for event_id in event_ids:\n",
    "        event_url = f'http://www.marathonguide.com/results/browse.cfm?MIDD={event_id}'\n",
    "        driver.get(event_url)\n",
    "        sleeptime = np.random.uniform(11, 14)\n",
    "        time.sleep(sleeptime)\n",
    "        \n",
    "        try:\n",
    "            # Find the event name, date, city, state, number of finishers, etc.\n",
    "            event_df.loc[event_id,'Event Name'] = re.findall(\n",
    "                r'<b>(.+)</b> - Results', driver.page_source)[0]\n",
    "            event_df.loc[event_id,'Date'] = re.findall(\n",
    "                r'[A-Z][a-z]+ \\d{1,2}, \\d{4}', driver.page_source)[1]\n",
    "            event_df.loc[event_id,['City','State']] = re.findall(\n",
    "                r'([^\\>]+), ([A-Z]{2})', page_source)[0] # Generalized for compound cities\n",
    "            event_df.loc[event_id,['Finishers','Males','Females']] = re.findall(\n",
    "                r'Finishers: (\\d+), Males - (\\d+) , Females - (\\d+)', driver.page_source)[0]\n",
    "            event_df.loc[event_id,['Male Win','Female Win']] = re.findall(\n",
    "                r'Male Winner: ([\\d:]+) \\| Female Winner: ([\\d:]+)', driver.page_source)[0]\n",
    "            event_df.loc[event_id,['Average Time','Time STD']] = re.findall(\n",
    "                r'Average Finish Time: ([\\d:]+) \\| STD: ([\\d:]+)', driver.page_source)[0]\n",
    "        \n",
    "        # In case the event page is missing some data, retrieve in opposite order to get any more data fields you can\n",
    "        except:\n",
    "            sleeptime = np.random.uniform(21, 24)\n",
    "            time.sleep(sleeptime)\n",
    "            try:\n",
    "                # Find the event name, date, city, state, number of finishers, etc.\n",
    "                event_df.loc[event_id,['Average Time','Time STD']] = re.findall(\n",
    "                    r'Average Finish Time: ([\\d:]+) \\| STD: ([\\d:]+)', driver.page_source)[0]\n",
    "                event_df.loc[event_id,['Male Win','Female Win']] = re.findall(\n",
    "                    r'Male Winner: ([\\d:]+) \\| Female Winner: ([\\d:]+)', driver.page_source)[0]\n",
    "                event_df.loc[event_id,['Finishers','Males','Females']] = re.findall(\n",
    "                    r'Finishers: (\\d+), Males - (\\d+) , Females - (\\d+)', driver.page_source)[0]\n",
    "                event_df.loc[event_id,['City','State']] = re.findall(\n",
    "                    r'([^\\>]+), ([A-Z]{2})', page_source)[0] # Generalized for compound cities\n",
    "                event_df.loc[event_id,'Date'] = re.findall(\n",
    "                    r'[A-Z][a-z]+ \\d{1,2}, \\d{4}', driver.page_source)[1]\n",
    "                event_df.loc[event_id,'Event Name'] = re.findall(\n",
    "                    r'<b>(.+)</b> - Results', driver.page_source)[0]\n",
    "            except:\n",
    "                # print(event_df.loc[event_id]) # Can print the missing rows if desired\n",
    "                pass\n",
    "\n",
    "    \n",
    "    # Add this event_df to the dictionary of events_dfs\n",
    "    events_dfs[event] = event_df\n",
    "    # Save the event results to a csv\n",
    "    directory = 'events_csvs'\n",
    "    if not os.path.exists(directory):\n",
    "        print(f'Directory created: {directory}')\n",
    "        os.mkdir(directory)\n",
    "\n",
    "    now = datetime.today().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    event_df.to_csv(f'{directory}/{event}_{now}.csv',index=True)\n",
    "    \n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cce8092-5a20-4051-b31b-c2c737c1c782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all events (all marathons, all years) into a larger df\n",
    "combined_df = pd.concat(events_dfs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db925c6-f90a-4f54-b166-0be8f487c19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined results to a csv\n",
    "directory = 'results_csvs'\n",
    "if not os.path.exists(directory):\n",
    "    print(f'Directory created: {directory}')\n",
    "    os.mkdir(directory)\n",
    "\n",
    "now = datetime.today().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "combined_df.to_csv(f'{directory}/race_results_{now}.csv',index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e881587f-c0cd-494d-a8cc-54be2240438f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Option C: Scrape marathon results with Selenium and BS4 (includes decile times)\n",
    "\n",
    "Warning: you should limit your data scraping using this method due to the high number of automated requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b9a88ae-f0f2-4fde-9f37-234ee6c89ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary where we'll add a df for each event\n",
    "events_dfs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ca6ec93-f9d9-4e2b-9909-3cfa3c571b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of initial race IDs for scraping MarathonGuide.com \n",
    "# Can find these on their website by clicking a particular race and year and looking at the end of the url.\n",
    "# Each event only needs race ID for one year added, since the race IDs for other years are automatically parsed.\n",
    "init_ids = {'Boston':'15000417',\n",
    "        'Chicago':'67001022',\n",
    "        'New York City':'472001105'\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fa2008-c83e-47e2-8639-a23dee6163de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scrape race results for all race events.\n",
    "# Increase time.sleep(x) in appropriate spot(s) if needed.\n",
    "\n",
    "for event, init_id in init_ids.items():\n",
    "    # Create driver and point to initial url\n",
    "    init_url = f'http://www.marathonguide.com/results/browse.cfm?MIDD={init_id}'\n",
    "    driver = webdriver.Safari()\n",
    "    driver.get(init_url)\n",
    "    sleeptime = np.random.uniform(6, 9)\n",
    "    time.sleep(sleeptime)\n",
    "    \n",
    "    # Find all the event IDs in the 'a' tag hyperlinks. Each corresponds to a year of the event.\n",
    "    event_ids = sorted(list(set(re.findall(r'MIDD=(\\d+)', driver.page_source))), reverse=True)\n",
    "    \n",
    "    # Create df for this event. First column is event ID.\n",
    "    event_df = (pd.DataFrame({'Event ID':event_ids})\n",
    "                .set_index('Event ID'))\n",
    "    \n",
    "    for event_id in event_ids:\n",
    "        event_url = f'http://www.marathonguide.com/results/browse.cfm?MIDD={event_id}'\n",
    "        driver.get(event_url)\n",
    "        sleeptime = np.random.uniform(6, 9)\n",
    "        time.sleep(sleeptime)\n",
    "        \n",
    "        try:\n",
    "            # Find the event name, date, city, state, number of finishers, etc.\n",
    "            event_df.loc[event_id,'Event Name'] = re.findall(\n",
    "                r'<b>(.+)</b> - Results', driver.page_source)[0]\n",
    "            event_df.loc[event_id,'Date'] = re.findall(\n",
    "                r'[A-Z][a-z]+ \\d{1,2}, \\d{4}', driver.page_source)[1]\n",
    "            event_df.loc[event_id,['City','State']] = re.findall(\n",
    "                r'([^\\>]+), ([A-Z]{2})', page_source)[0] # Generalized for compound cities\n",
    "            event_df.loc[event_id,['Finishers','Males','Females']] = re.findall(\n",
    "                r'Finishers: (\\d+), Males - (\\d+) , Females - (\\d+)', driver.page_source)[0]\n",
    "            event_df.loc[event_id,['Male Win','Female Win']] = re.findall(\n",
    "                r'Male Winner: ([\\d:]+) \\| Female Winner: ([\\d:]+)', driver.page_source)[0]\n",
    "            event_df.loc[event_id,['Average Time','Time STD']] = re.findall(\n",
    "                r'Average Finish Time: ([\\d:]+) \\| STD: ([\\d:]+)', driver.page_source)[0]\n",
    "        \n",
    "        # In case the event page is missing some data, only get deciles if we have finisher count\n",
    "        except:\n",
    "            print(event_df.loc[event_id])\n",
    "            if np.isnan(float(event_df.loc[event_id,'Finishers'])): continue\n",
    "            sleeptime = np.random.uniform(6, 9)\n",
    "            time.sleep(sleeptime)\n",
    "            pass\n",
    "        \n",
    "        # Calculate Decile Places\n",
    "        finishers = float(event_df.loc[event_id,'Finishers'])\n",
    "        decile_places = [str(int(decile)) for decile in np.linspace(1,finishers,11)]\n",
    "        event_df.loc[event_id,['D0 Place','D1 Place','D2 Place','D3 Place','D4 Place','D5 Place',\n",
    "                              'D6 Place','D7 Place','D8 Place','D9 Place', 'D10 Place']\n",
    "                    ] = decile_places\n",
    "        \n",
    "        # Names for cols for decile times we'll iterate over\n",
    "        decile_cols = ['D0','D1','D2','D3','D4','D5','D6','D7','D8','D9','D10']\n",
    "        \n",
    "        # Find the finish time for each decile\n",
    "        for finish_place, decile in zip(decile_places,decile_cols):\n",
    "            try:\n",
    "                # Find the dropdown/Select elements for finisher place range/options\n",
    "                selector = driver.find_element(by=By.XPATH, value=\"//select[@name='RaceRange']\")\n",
    "                all_options = selector.find_elements(by=By.TAG_NAME, value=\"option\")\n",
    "                results_page = int((int(finish_place)+99)/100)\n",
    "                option = all_options[results_page]\n",
    "                submit_button = driver.find_element(by=By.XPATH, value=\"//input[@name='SubmitButton']\")\n",
    "\n",
    "                # Load the results page\n",
    "                selector.click()\n",
    "                option.click()\n",
    "                submit_button.click()\n",
    "                sleeptime = np.random.uniform(6, 9)\n",
    "                time.sleep(sleeptime)\n",
    "\n",
    "                # The results page contains several levels of nested tables.\n",
    "                # Depending on the particular page, here are possible paths\n",
    "                try:\n",
    "                    soup = BeautifulSoup(driver.page_source)\n",
    "                    table1 = soup.find_all('table')[9]\n",
    "                    table2 = table1.find_all('table')[3]\n",
    "                    table3 = table2.find_all('table')[0]\n",
    "                    table4 = table3.find_all('table')[0]\n",
    "                    table5 = table4.find_all('table')[0]\n",
    "                except:\n",
    "                    soup = BeautifulSoup(driver.page_source)\n",
    "                    table1 = soup.find_all('table')[11]\n",
    "                    table2 = table1.find_all('table')[3]\n",
    "                    table3 = table2.find_all('table')[0]\n",
    "                    table4 = table3.find_all('table')[0]\n",
    "                    table5 = table4.find_all('table')[0]\n",
    "\n",
    "                # Convert the table into pandas df and clean\n",
    "                page_df = pd.read_html(str(table5))[0].iloc[2:]\n",
    "                page_df.columns = page_df.iloc[0]\n",
    "                page_df = page_df.drop(index = 2)\n",
    "                # Account for different column naming per event\n",
    "                if 'Net Time' not in page_df.columns:\n",
    "                    page_df = page_df.rename(columns={'Time':'Net Time'})\n",
    "\n",
    "                # Get the finish time for the desired finish place and update event_df\n",
    "                finish_time = (page_df[page_df['OverAllPlace'] == finish_place]\n",
    "                               .loc[:,'Net Time']\n",
    "                               .iloc[0])\n",
    "                event_df.loc[event_id,decile] = finish_time\n",
    "                \n",
    "                # Return to event page to prep for next decile\n",
    "                driver.get(event_url)\n",
    "                sleeptime = np.random.uniform(6, 9)\n",
    "                time.sleep(sleeptime)\n",
    "                \n",
    "            # If parsing a decile time fails:\n",
    "            # print data, leave decile time as NaN, and continue to remaining deciles\n",
    "            except:\n",
    "                print(event_df.loc[event_id,['Event Name','Date']])\n",
    "                print(f'    {decile}: {finish_place}\\n')\n",
    "                sleeptime = np.random.uniform(6, 9)\n",
    "                time.sleep(sleeptime)\n",
    "                pass\n",
    "            \n",
    "                # Return to event page to prep for next decile\n",
    "                driver.get(event_url)\n",
    "                sleeptime = np.random.uniform(6, 9)\n",
    "                time.sleep(sleeptime)\n",
    "    \n",
    "    # Add this event_df to the dictionary of events_dfs\n",
    "    events_dfs[event] = event_df\n",
    "    # Save the event results to a csv\n",
    "    directory = 'events_csvs'\n",
    "    if not os.path.exists(directory):\n",
    "        print(f'Directory created: {directory}')\n",
    "        os.mkdir(directory)\n",
    "\n",
    "    now = datetime.today().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    event_df.to_csv(f'{directory}/{event}_{now}.csv',index=True)\n",
    "    \n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56ee6a3-5b53-4705-a68a-d99b02cdf82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check race results csvs for missing data and try pulling just those rows again\n",
    "\n",
    "for file in os.listdir(\"results_csvs\"):\n",
    "    if file.endswith(\".csv\"):\n",
    "        print(os.path.join(\"results_csvs\", file))\n",
    "        \n",
    "        # Open a df and get the index of all rows with one or more missing cells (NaN)\n",
    "        event_df = (pd.read_csv(os.path.join(\"results_csvs\", file))\n",
    "                   .set_index('Event ID'))\n",
    "        nan_rows = event_df[event_df.isnull().T.any()]\n",
    "        print(nan_rows.index)\n",
    "        \n",
    "        for event_id in nan_rows.index:\n",
    "            event_url = f'http://webcache.googleusercontent.com/search?q=cache:http://www.marathonguide.com/results/browse.cfm?MIDD={event_id}'\n",
    "            # Create driver and point to url\n",
    "            driver = webdriver.Safari()\n",
    "            driver.get(event_url)\n",
    "            sleeptime = np.random.uniform(6, 9)\n",
    "            time.sleep(sleeptime)\n",
    "\n",
    "            try:\n",
    "                # Find the event name, date, city, state, number of finishers, etc.\n",
    "                event_df.loc[event_id,'Event Name'] = re.findall(\n",
    "                    r'<b>(.+)</b> - Results', driver.page_source)[0]\n",
    "                event_df.loc[event_id,'Date'] = re.findall(\n",
    "                    r'[A-Z][a-z]+ \\d{1,2}, \\d{4}', driver.page_source)[1]\n",
    "                event_df.loc[event_id,['City','State']] = re.findall(\n",
    "                    r'([\\w\\- ]+), ([A-Z]{2})', driver.page_source)[0]\n",
    "                event_df.loc[event_id,['Finishers','Males','Females']] = re.findall(\n",
    "                    r'Finishers: (\\d+), Males - (\\d+) , Females - (\\d+)', driver.page_source)[0]\n",
    "                event_df.loc[event_id,['Male Win','Female Win']] = re.findall(\n",
    "                    r'Male Winner: ([\\d:]+) \\| Female Winner: ([\\d:]+)', driver.page_source)[0]\n",
    "                event_df.loc[event_id,['Average Time','Time STD']] = re.findall(\n",
    "                    r'Average Finish Time: ([\\d:]+) \\| STD: ([\\d:]+)', driver.page_source)[0]\n",
    "\n",
    "            # In case the event page is missing some data, only get deciles if we have finisher count\n",
    "            except:\n",
    "                print(event_df.loc[event_id])\n",
    "                if np.isnan(float(event_df.loc[event_id,'Finishers'])): continue\n",
    "                sleeptime = np.random.uniform(6, 9)\n",
    "                time.sleep(sleeptime)\n",
    "                pass\n",
    "\n",
    "            # Calculate Decile Places\n",
    "            finishers = float(event_df.loc[event_id,'Finishers'])\n",
    "            decile_places = [str(int(decile)) for decile in np.linspace(1,finishers,11)]\n",
    "            event_df.loc[event_id,['D0 Place','D1 Place','D2 Place','D3 Place','D4 Place','D5 Place',\n",
    "                                  'D6 Place','D7 Place','D8 Place','D9 Place', 'D10 Place']\n",
    "                        ] = decile_places\n",
    "\n",
    "            # Names for cols for decile times we'll iterate over\n",
    "            decile_cols = ['D0','D1','D2','D3','D4','D5','D6','D7','D8','D9','D10']\n",
    "\n",
    "            # Find the finish time for each decile\n",
    "            for finish_place, decile in zip(decile_places,decile_cols): # [0:3] REMOVE these indices to get all deciles\n",
    "                try:\n",
    "                    # Find the dropdown/Select elements for finisher place range/options\n",
    "                    selector = driver.find_element(by=By.XPATH, value=\"//select[@name='RaceRange']\")\n",
    "                    all_options = selector.find_elements(by=By.TAG_NAME, value=\"option\")\n",
    "                    results_page = int((int(finish_place)+99)/100)\n",
    "                    option = all_options[results_page]\n",
    "                    submit_button = driver.find_element(by=By.XPATH, value=\"//input[@name='SubmitButton']\")\n",
    "\n",
    "                    # Load the results page\n",
    "                    selector.click()\n",
    "                    option.click()\n",
    "                    submit_button.click()\n",
    "                    sleeptime = np.random.uniform(6, 9)\n",
    "                    time.sleep(sleeptime)\n",
    "\n",
    "                    # The results page contains several levels of nested tables.\n",
    "                    # Depending on the particular page, here are possible paths\n",
    "                    try:\n",
    "                        soup = BeautifulSoup(driver.page_source)\n",
    "                        table1 = soup.find_all('table')[9]\n",
    "                        table2 = table1.find_all('table')[3]\n",
    "                        table3 = table2.find_all('table')[0]\n",
    "                        table4 = table3.find_all('table')[0]\n",
    "                        table5 = table4.find_all('table')[0]\n",
    "                    except:\n",
    "                        soup = BeautifulSoup(driver.page_source)\n",
    "                        table1 = soup.find_all('table')[11]\n",
    "                        table2 = table1.find_all('table')[3]\n",
    "                        table3 = table2.find_all('table')[0]\n",
    "                        table4 = table3.find_all('table')[0]\n",
    "                        table5 = table4.find_all('table')[0]\n",
    "\n",
    "                    # Convert the table into pandas df and clean\n",
    "                    page_df = pd.read_html(str(table5))[0].iloc[2:]\n",
    "                    page_df.columns = page_df.iloc[0]\n",
    "                    page_df = page_df.drop(index = 2)\n",
    "                    # Account for different column naming per event\n",
    "                    if 'Net Time' not in page_df.columns:\n",
    "                        page_df = page_df.rename(columns={'Time':'Net Time'})\n",
    "\n",
    "                    # Get the finish time for the desired finish place and update event_df\n",
    "                    finish_time = (page_df[page_df['OverAllPlace'] == finish_place]\n",
    "                                   .loc[:,'Net Time']\n",
    "                                   .iloc[0])\n",
    "                    event_df.loc[event_id,decile] = finish_time\n",
    "\n",
    "                    # Return to event page to prep for next decile\n",
    "                    driver.get(event_url)\n",
    "                    sleeptime = np.random.uniform(6, 9)\n",
    "                    time.sleep(sleeptime)\n",
    "\n",
    "                # If parsing a decile time fails:\n",
    "                # print data, leave decile time as NaN, and continue to remaining deciles\n",
    "                except:\n",
    "                    print(event_df.loc[event_id,['Event Name','Date']])\n",
    "                    print(f'    {decile}: {finish_place}\\n')\n",
    "                    sleeptime = np.random.uniform(6, 9)\n",
    "                    time.sleep(sleeptime)\n",
    "                    pass\n",
    "\n",
    "                    # Return to event page to prep for next decile\n",
    "                    driver.get(event_url)\n",
    "                    sleeptime = np.random.uniform(6, 9)\n",
    "                    time.sleep(sleeptime)\n",
    "            driver.close()\n",
    "\n",
    "        # Update this event_df in the dictionary of events_dfs\n",
    "        events_dfs[event] = event_df\n",
    "        # Save the event results to a csv\n",
    "        directory = 'events_csvs_cleaned'\n",
    "        if not os.path.exists(directory):\n",
    "            print(f'Directory created: {directory}')\n",
    "            os.mkdir(directory)\n",
    "\n",
    "        event = event_df.loc[:,'Event Name'].iloc[0]\n",
    "        now = datetime.today().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "        event_df.to_csv(f'{directory}/{event}_{now}.csv',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c21117c-6903-4b29-a8c5-f2aab8881f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all events (all marathons, all years) into a larger df\n",
    "combined_df = pd.concat(events_dfs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d4d572-0512-48c6-b535-d60d9af03012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined results to a csv\n",
    "directory = 'results_csvs'\n",
    "if not os.path.exists(directory):\n",
    "    print(f'Directory created: {directory}')\n",
    "    os.mkdir(directory)\n",
    "\n",
    "now = datetime.today().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "combined_df.to_csv(f'{directory}/race_results_{now}.csv',index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7829af-9fbb-4bc1-966e-919aa2c0699e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3. Get elevation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0366539-2f80-4926-9eaa-828f8f285319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Big Sur International Marathon', 'Boston Marathon', 'California International Marathon', 'Chicago Marathon', 'Disney World Marathon', \"Grandma's Marathon\", 'Honolulu Marathon', 'Houston Marathon', 'L.A. Marathon', 'Marine Corps Marathon', 'Montreal International Marathon', 'New York City Marathon', 'Ottawa Marathon', 'Philadelphia Marathon', 'Portland Marathon', \"Rock 'n' Roll Arizona Marathon\", \"Rock 'n' Roll Montreal Marathon\", \"Rock 'n' Roll San Diego Marathon\", \"Rock 'n' Roll Seattle Marathon\", 'San Francisco Marathon', 'Seattle Marathon', 'St. George Marathon', 'Toronto Waterfront Marathon', 'Twin Cities Marathon', 'Vancouver International Marathon']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(event_df['Event Name'].unique()))\n",
    "\n",
    "elevation_columns = ['Event Name','Elev Gain','Elev Loss','Elev Max','Elev Min'] # units of feet\n",
    "\n",
    "# Data from https://findmymarathon.com/ \n",
    "elevation_metrics = {'Big Sur International Marathon':[1654,1949,578,8],\n",
    "                     'Boston Marathon':[815,1275,470,10],\n",
    "                     'California International Marathon':[663,1003,359,19],\n",
    "                     'Chicago Marathon':[243,242,612,580],\n",
    "                     'Disney World Marathon':[351,354,105,75],\n",
    "                     \"Grandma's Marathon\":[471,581,727,603],\n",
    "                     'Honolulu Marathon':[457,455,126,3],\n",
    "                     'Houston Marathon':[225,222,84,27],\n",
    "                     'L.A. Marathon':[943,1169,566,203],\n",
    "                     'Marine Corps Marathon':[630,593,232,2],\n",
    "                     'New York City Marathon':[810,824,260,7],\n",
    "                     'Philadelphia Marathon':[846,820,148,5],\n",
    "                     'Portland Marathon':[873,873,178,30],\n",
    "                     \"Rock 'n' Roll Arizona Marathon\":[509,516,1277,1151],\n",
    "                     \"Rock 'n' Roll San Diego Marathon\":[798,1031,403,8],\n",
    "                     \"Rock 'n' Roll Seattle Marathon\":[1149,1155,404,17],\n",
    "                     'San Francisco Marathon':[1365,1364,306,5],\n",
    "                     'Seattle Marathon':[931,929,192,21],\n",
    "                     'St. George Marathon':[500,3057,5244,2685],\n",
    "                     'Twin Cities Marathon':[576,495,950,788]\n",
    "                     }\n",
    "\n",
    "elev_df = pd.DataFrame(elevation_metrics).T.reset_index()\n",
    "elev_df.columns = elevation_columns\n",
    "\n",
    "# Save a copy of the elevation data\n",
    "directory = 'elevation_csvs'\n",
    "if not os.path.exists(directory):\n",
    "    print(f'Directory created: {directory}')\n",
    "    os.mkdir(directory)\n",
    "\n",
    "now = datetime.today().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "elev_df.to_csv(f'{directory}/elevations_{now}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3aa6d2b-70e7-4adf-bba9-112805f4ca3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Event Name</th>\n",
       "      <th>Elev Gain</th>\n",
       "      <th>Elev Loss</th>\n",
       "      <th>Elev Max</th>\n",
       "      <th>Elev Min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Big Sur International Marathon</td>\n",
       "      <td>1654</td>\n",
       "      <td>1949</td>\n",
       "      <td>578</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Boston Marathon</td>\n",
       "      <td>815</td>\n",
       "      <td>1275</td>\n",
       "      <td>470</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>California International Marathon</td>\n",
       "      <td>663</td>\n",
       "      <td>1003</td>\n",
       "      <td>359</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chicago Marathon</td>\n",
       "      <td>243</td>\n",
       "      <td>242</td>\n",
       "      <td>612</td>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Disney World Marathon</td>\n",
       "      <td>351</td>\n",
       "      <td>354</td>\n",
       "      <td>105</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Grandma's Marathon</td>\n",
       "      <td>471</td>\n",
       "      <td>581</td>\n",
       "      <td>727</td>\n",
       "      <td>603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Honolulu Marathon</td>\n",
       "      <td>457</td>\n",
       "      <td>455</td>\n",
       "      <td>126</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Houston Marathon</td>\n",
       "      <td>225</td>\n",
       "      <td>222</td>\n",
       "      <td>84</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>L.A. Marathon</td>\n",
       "      <td>943</td>\n",
       "      <td>1169</td>\n",
       "      <td>566</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Marine Corps Marathon</td>\n",
       "      <td>630</td>\n",
       "      <td>593</td>\n",
       "      <td>232</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>New York City Marathon</td>\n",
       "      <td>810</td>\n",
       "      <td>824</td>\n",
       "      <td>260</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Philadelphia Marathon</td>\n",
       "      <td>846</td>\n",
       "      <td>820</td>\n",
       "      <td>148</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Portland Marathon</td>\n",
       "      <td>873</td>\n",
       "      <td>873</td>\n",
       "      <td>178</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Rock 'n' Roll Arizona Marathon</td>\n",
       "      <td>509</td>\n",
       "      <td>516</td>\n",
       "      <td>1277</td>\n",
       "      <td>1151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Rock 'n' Roll San Diego Marathon</td>\n",
       "      <td>798</td>\n",
       "      <td>1031</td>\n",
       "      <td>403</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Rock 'n' Roll Seattle Marathon</td>\n",
       "      <td>1149</td>\n",
       "      <td>1155</td>\n",
       "      <td>404</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>San Francisco Marathon</td>\n",
       "      <td>1365</td>\n",
       "      <td>1364</td>\n",
       "      <td>306</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Seattle Marathon</td>\n",
       "      <td>931</td>\n",
       "      <td>929</td>\n",
       "      <td>192</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>St. George Marathon</td>\n",
       "      <td>500</td>\n",
       "      <td>3057</td>\n",
       "      <td>5244</td>\n",
       "      <td>2685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Twin Cities Marathon</td>\n",
       "      <td>576</td>\n",
       "      <td>495</td>\n",
       "      <td>950</td>\n",
       "      <td>788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Event Name  Elev Gain  Elev Loss  Elev Max  \\\n",
       "0      Big Sur International Marathon       1654       1949       578   \n",
       "1                     Boston Marathon        815       1275       470   \n",
       "2   California International Marathon        663       1003       359   \n",
       "3                    Chicago Marathon        243        242       612   \n",
       "4               Disney World Marathon        351        354       105   \n",
       "5                  Grandma's Marathon        471        581       727   \n",
       "6                   Honolulu Marathon        457        455       126   \n",
       "7                    Houston Marathon        225        222        84   \n",
       "8                       L.A. Marathon        943       1169       566   \n",
       "9               Marine Corps Marathon        630        593       232   \n",
       "10             New York City Marathon        810        824       260   \n",
       "11              Philadelphia Marathon        846        820       148   \n",
       "12                  Portland Marathon        873        873       178   \n",
       "13     Rock 'n' Roll Arizona Marathon        509        516      1277   \n",
       "14   Rock 'n' Roll San Diego Marathon        798       1031       403   \n",
       "15     Rock 'n' Roll Seattle Marathon       1149       1155       404   \n",
       "16             San Francisco Marathon       1365       1364       306   \n",
       "17                   Seattle Marathon        931        929       192   \n",
       "18                St. George Marathon        500       3057      5244   \n",
       "19               Twin Cities Marathon        576        495       950   \n",
       "\n",
       "    Elev Min  \n",
       "0          8  \n",
       "1         10  \n",
       "2         19  \n",
       "3        580  \n",
       "4         75  \n",
       "5        603  \n",
       "6          3  \n",
       "7         27  \n",
       "8        203  \n",
       "9          2  \n",
       "10         7  \n",
       "11         5  \n",
       "12        30  \n",
       "13      1151  \n",
       "14         8  \n",
       "15        17  \n",
       "16         5  \n",
       "17        21  \n",
       "18      2685  \n",
       "19       788  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elev_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9530da-02b0-4e7d-a226-7ec99cc0b7b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 4. Pull Weather Data\n",
    "\n",
    "1. Define a couple of useful functions\n",
    "2. Open marathon results df to get cities and dates\n",
    "3. Pull weather from NOAA NCEI\n",
    "4. Save Weather df to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5569765-5f26-414a-aed2-aeef83bfddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source of Weather Data:\n",
    "\n",
    "# NCEI Global Summary of the Day:\n",
    "# https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ncdc:C00516\n",
    "\n",
    "# Data types: https://www.ncei.noaa.gov/data/global-summary-of-the-day/doc/readme.txt\n",
    "# TEMP - Mean temperature (.1 Fahrenheit)\n",
    "# DEWP - Mean dew point (.1 Fahrenheit)\n",
    "# SLP - Mean sea level pressure (.1 mb)\n",
    "# STP - Mean station pressure (.1 mb)\n",
    "# VISIB - Mean visibility (.1 miles)\n",
    "# WDSP – Mean wind speed (.1 knots)\n",
    "# MXSPD - Maximum sustained wind speed (.1 knots)\n",
    "# GUST - Maximum wind gust (.1 knots)\n",
    "# MAX - Maximum temperature (.1 Fahrenheit)\n",
    "# MIN - Minimum temperature (.1 Fahrenheit)\n",
    "# PRCP - Precipitation amount (.01 inches)\n",
    "# SNDP - Snow depth (.1 inches)\n",
    "# FRSHTT – Indicator for occurrence of:\n",
    "#                               Fog\n",
    "#                               Rain or Drizzle\n",
    "#                               Snow or Ice Pellets\n",
    "#                               Hail\n",
    "#                               Thunder\n",
    "#                               Tornado/Funnel Cloud\n",
    "\n",
    "# Find Stations: https://www.ncei.noaa.gov/maps/daily/?layers=0001\n",
    "# Generally used closest airport since data usually covers full range of years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1036c7b-aedc-4198-b5ad-1a316b2f096f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Weather Stations\n",
    "station_ids = {'Boston':['72509014739'],\n",
    "               'Carmel':['72491523259','72491599999','72491523245'],\n",
    "               'Chicago':['72530094846'],\n",
    "               'Duluth':['72745014913'],\n",
    "               'Honololu':['91182022521'],\n",
    "               'Houston':['72244012918','72243012960'],\n",
    "               'Los Angeles':['72288593197','72288599999','72287493134'],\n",
    "               'Minneapolis':['72658014922'],\n",
    "               'Montreal':['71371099999','71612099999','71627099999','71627094792'],\n",
    "               'New York City':['72503014732', '99999914732'],\n",
    "               'Orlando':['72205012815'],\n",
    "               'Ottawa':['71628099999','71627999999'],\n",
    "               'Philadelphia':['72408013739'],\n",
    "               'Phoenix':['72278023183'],\n",
    "               'Portland':['72698024229'],\n",
    "               'Sacramento':['72483993225','72483999999','72483323206','72483399999'],\n",
    "               'San Diego':['72290023188'],\n",
    "               'San Francisco':['72494023234'],\n",
    "               'Seattle':['72793524234','72793024233'],\n",
    "               'St. George':['72475423186','72475499999','72092299999','72475593129'],\n",
    "               'Toronto':['71265099999','71624099999'],\n",
    "               'Vancouver':['71892099999','71784099999'],\n",
    "               'Washington':['72405013743'],\n",
    "               'Berlin':['10382099999'],'London':['3772099999'],'Tokyo':['47662099999']}\n",
    "\n",
    "# Weather data types we'll keep\n",
    "weather_cols = ['STATION','DATE','LATITUDE','LONGITUDE','ELEVATION',\n",
    "                'TEMP','MAX','MIN','PRCP','DEWP',\n",
    "                'WDSP','SLP','STP','VISIB']\n",
    "\n",
    "# Main function for getting NCEI weather data\n",
    "# Note that the NOAA files are downloaded and saved to the local drive\n",
    "# This makes future runs faster for you and reduces hits on the database server\n",
    "def get_weather(date, city):\n",
    "    try:\n",
    "        stations = station_ids[city]\n",
    "        year = date.year\n",
    "        directory = f'weather_csvs/{city}/'\n",
    "        if not os.path.exists(directory):\n",
    "            print(f'Created directory {directory}.')\n",
    "            os.mkdir(directory)\n",
    "        # Try multiple station_ids for the same location since ids are occasionally updated\n",
    "        for station_id in station_ids[city]:\n",
    "            file_name = f'{station_id}_{year}.csv'\n",
    "            try:\n",
    "                try: # Check if file saved locally from previous run\n",
    "                    temp_df = pd.read_csv(directory+file_name)\n",
    "                except:\n",
    "                    url = f'https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/{year}/{station_id}.csv'\n",
    "                    temp_df = pd.read_csv(url)\n",
    "                    temp_df.to_csv(f'{directory}{file_name}',index=False)\n",
    "                    print(f'Retrieved {file_name} online and saved to local drive folder {directory}.')\n",
    "                    sleeptime = np.random.uniform(1, 2)\n",
    "                    time.sleep(sleeptime)\n",
    "                temp_df['DATE'] = pd.to_datetime(temp_df['DATE'], format=\"%Y-%m-%d\")\n",
    "                weather_data = temp_df[temp_df['DATE']==date]\n",
    "                weather_data = weather_data[weather_cols].squeeze().tolist()\n",
    "                break\n",
    "            except: continue # on to check the next station in this city\n",
    "            \n",
    "        temp_df['DATE'] = pd.to_datetime(temp_df['DATE'], format=\"%Y-%m-%d\")\n",
    "        weather_data = temp_df[temp_df['DATE']==date]\n",
    "        weather_data = weather_data[weather_cols].squeeze().tolist()\n",
    "    except: # If function fails, get empty weather data (NCEI records don't go back far enough)\n",
    "        weather_data = pd.DataFrame(columns = weather_cols,\n",
    "                                    index = [0]).squeeze().tolist()\n",
    "    return weather_data\n",
    "\n",
    "# Secondary weather function to calculate relative humidity based on temperature and dewpoint (degrees F)\n",
    "def calc_humidity(temp_f,dewpoint_f):\n",
    "    # Convert temps from F to C\n",
    "    T = (temp_f-32)*5/9\n",
    "    DP = (dewpoint_f-32)*5/9\n",
    "    \n",
    "    # Save constants\n",
    "    a = 17.625\n",
    "    b = 243.04\n",
    "    \n",
    "    # Formula from https://bmcnoldy.rsmas.miami.edu/Humidity.html\n",
    "    RH = 100*(math.exp((a*DP)/(b+DP))/math.exp((a*T)/(b+T)))\n",
    "    return RH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f8bc81f-9d90-4255-98c0-75478a805a99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[71784099999,\n",
       " Timestamp('2005-05-01 00:00:00'),\n",
       " 49.35,\n",
       " -123.2,\n",
       " 168.0,\n",
       " 54.6,\n",
       " 66.2,\n",
       " 46.4,\n",
       " 0.07,\n",
       " 49.1,\n",
       " 1.7,\n",
       " 1020.8,\n",
       " 0.5,\n",
       " 999.9]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test get_weather\n",
    "get_weather(pd.to_datetime('2005-05-01', format=\"%Y-%m-%d\"), 'Vancouver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b77398a7-3503-49e7-8b6c-20dc7acb320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load marathon results\n",
    "directory = 'results_csvs'\n",
    "newest_results = [file for file in sorted(os.listdir(directory)) if file.endswith('.csv')][-1]\n",
    "event_df = pd.read_csv(os.path.join(directory, newest_results))\n",
    "# print(os.path.join(directory, newest_results))\n",
    "event_df['Date'] = pd.to_datetime(event_df['Date'])\n",
    "\n",
    "# Pull the weather data according to Date and City\n",
    "event_df[weather_cols] = event_df.apply(lambda x: get_weather(x.Date, x.City), axis=1, result_type='expand')\n",
    "event_df['RELHUM'] = event_df.apply(lambda x: calc_humidity(x.TEMP, x.DEWP), axis=1)\n",
    "    \n",
    "weather_df = (event_df[['Date', 'City', 'State', 'STATION', \n",
    "                       'LATITUDE', 'LONGITUDE', 'ELEVATION', \n",
    "                       'TEMP', 'MAX', 'MIN', 'PRCP', 'DEWP', 'WDSP', \n",
    "                       'SLP', 'STP', 'VISIB', 'RELHUM']]\n",
    "              .sort_values(['City','Date']))\n",
    "\n",
    "weather_df['PRCP'].replace({99.99:0}, inplace=True) # NOAA database uses all 9's for unrecorded data\n",
    "\n",
    "# Save a copy of the weather data\n",
    "directory = 'weather_table_csvs'\n",
    "if not os.path.exists(directory):\n",
    "    os.mkdir(directory)\n",
    "    print(f'Directory created: {directory}')\n",
    "\n",
    "now = datetime.today().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "weather_df.to_csv(f'{directory}/weather_{now}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a361a15-35e5-4a18-9e41-f94000778748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATION</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ELEVATION</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>MAX</th>\n",
       "      <th>MIN</th>\n",
       "      <th>PRCP</th>\n",
       "      <th>DEWP</th>\n",
       "      <th>WDSP</th>\n",
       "      <th>SLP</th>\n",
       "      <th>STP</th>\n",
       "      <th>VISIB</th>\n",
       "      <th>RELHUM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.790000e+02</td>\n",
       "      <td>479.000000</td>\n",
       "      <td>479.000000</td>\n",
       "      <td>479.000000</td>\n",
       "      <td>479.000000</td>\n",
       "      <td>479.000000</td>\n",
       "      <td>479.000000</td>\n",
       "      <td>479.000000</td>\n",
       "      <td>479.000000</td>\n",
       "      <td>479.000000</td>\n",
       "      <td>479.000000</td>\n",
       "      <td>479.000000</td>\n",
       "      <td>479.000000</td>\n",
       "      <td>479.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.316054e+10</td>\n",
       "      <td>39.247881</td>\n",
       "      <td>-102.064404</td>\n",
       "      <td>111.088079</td>\n",
       "      <td>57.139040</td>\n",
       "      <td>67.765762</td>\n",
       "      <td>48.072234</td>\n",
       "      <td>0.070188</td>\n",
       "      <td>44.995825</td>\n",
       "      <td>6.877035</td>\n",
       "      <td>1336.602505</td>\n",
       "      <td>404.716701</td>\n",
       "      <td>13.972025</td>\n",
       "      <td>66.331074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.881581e+09</td>\n",
       "      <td>6.874670</td>\n",
       "      <td>23.069471</td>\n",
       "      <td>213.862844</td>\n",
       "      <td>9.624765</td>\n",
       "      <td>10.903297</td>\n",
       "      <td>9.934307</td>\n",
       "      <td>0.204460</td>\n",
       "      <td>11.590381</td>\n",
       "      <td>3.400088</td>\n",
       "      <td>1663.580462</td>\n",
       "      <td>476.808757</td>\n",
       "      <td>63.986119</td>\n",
       "      <td>16.776645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.126510e+10</td>\n",
       "      <td>21.324000</td>\n",
       "      <td>-157.939460</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>31.100000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>989.700000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>14.356391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.227802e+10</td>\n",
       "      <td>34.017000</td>\n",
       "      <td>-121.845300</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>50.700000</td>\n",
       "      <td>60.100000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.700000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>1014.100000</td>\n",
       "      <td>13.800000</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>56.789433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.248399e+10</td>\n",
       "      <td>39.873270</td>\n",
       "      <td>-95.360000</td>\n",
       "      <td>13.400000</td>\n",
       "      <td>57.500000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46.100000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>1017.900000</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>69.686268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.253009e+10</td>\n",
       "      <td>45.322500</td>\n",
       "      <td>-79.383333</td>\n",
       "      <td>113.395000</td>\n",
       "      <td>64.050000</td>\n",
       "      <td>75.900000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>53.550000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1022.600000</td>\n",
       "      <td>994.450000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>77.992114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.118202e+10</td>\n",
       "      <td>49.350000</td>\n",
       "      <td>-71.009700</td>\n",
       "      <td>1702.600000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>2.240000</td>\n",
       "      <td>68.900000</td>\n",
       "      <td>23.300000</td>\n",
       "      <td>9999.900000</td>\n",
       "      <td>999.900000</td>\n",
       "      <td>999.900000</td>\n",
       "      <td>97.841783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            STATION    LATITUDE   LONGITUDE    ELEVATION        TEMP  \\\n",
       "count  4.790000e+02  479.000000  479.000000   479.000000  479.000000   \n",
       "mean   7.316054e+10   39.247881 -102.064404   111.088079   57.139040   \n",
       "std    3.881581e+09    6.874670   23.069471   213.862844    9.624765   \n",
       "min    7.126510e+10   21.324000 -157.939460     1.900000   31.100000   \n",
       "25%    7.227802e+10   34.017000 -121.845300     3.700000   50.700000   \n",
       "50%    7.248399e+10   39.873270  -95.360000    13.400000   57.500000   \n",
       "75%    7.253009e+10   45.322500  -79.383333   113.395000   64.050000   \n",
       "max    9.118202e+10   49.350000  -71.009700  1702.600000   80.000000   \n",
       "\n",
       "              MAX         MIN        PRCP        DEWP        WDSP  \\\n",
       "count  479.000000  479.000000  479.000000  479.000000  479.000000   \n",
       "mean    67.765762   48.072234    0.070188   44.995825    6.877035   \n",
       "std     10.903297    9.934307    0.204460   11.590381    3.400088   \n",
       "min     39.000000   23.000000    0.000000    4.600000    1.100000   \n",
       "25%     60.100000   41.000000    0.000000   36.700000    4.400000   \n",
       "50%     68.000000   48.000000    0.000000   46.100000    6.200000   \n",
       "75%     75.900000   55.000000    0.030000   53.550000    9.000000   \n",
       "max     95.000000   73.000000    2.240000   68.900000   23.300000   \n",
       "\n",
       "               SLP         STP       VISIB      RELHUM  \n",
       "count   479.000000  479.000000  479.000000  479.000000  \n",
       "mean   1336.602505  404.716701   13.972025   66.331074  \n",
       "std    1663.580462  476.808757   63.986119   16.776645  \n",
       "min     989.700000    0.300000    0.600000   14.356391  \n",
       "25%    1014.100000   13.800000    9.200000   56.789433  \n",
       "50%    1017.900000   22.500000    9.900000   69.686268  \n",
       "75%    1022.600000  994.450000   10.000000   77.992114  \n",
       "max    9999.900000  999.900000  999.900000   97.841783  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the output\n",
    "weather_df.describe()\n",
    "\n",
    "# In this NOAA database, if data item is unrecorded/missing then all digits will be 9's.\n",
    "# You may want to drop rows with bad data.\n",
    "# We'll drop entire columns for SLP and STP since those have lots of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c4bb02c-0cb5-4f90-9b07-71a633192117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">PRCP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>median</th>\n",
       "      <th>mean</th>\n",
       "      <th>amax</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>City</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>St. George</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>San Francisco</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phoenix</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001765</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>San Diego</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001905</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ottawa</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.021579</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carmel</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.014762</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Los Angeles</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.014091</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York City</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Orlando</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.048182</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chicago</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.050476</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vancouver</th>\n",
       "      <td>0.020</td>\n",
       "      <td>0.084737</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seattle</th>\n",
       "      <td>0.015</td>\n",
       "      <td>0.077813</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Minneapolis</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.047143</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Houston</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.072273</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portland</th>\n",
       "      <td>0.010</td>\n",
       "      <td>0.094762</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Toronto</th>\n",
       "      <td>0.015</td>\n",
       "      <td>0.135000</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sacramento</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Philadelphia</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.067143</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Montreal</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Washington</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Duluth</th>\n",
       "      <td>0.020</td>\n",
       "      <td>0.177143</td>\n",
       "      <td>1.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Honololu</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.139048</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Boston</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.147273</td>\n",
       "      <td>2.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                PRCP                \n",
       "              median      mean  amax\n",
       "City                                \n",
       "St. George     0.000  0.000000  0.00\n",
       "San Francisco  0.000  0.000000  0.00\n",
       "Phoenix        0.000  0.001765  0.03\n",
       "San Diego      0.000  0.001905  0.04\n",
       "Ottawa         0.000  0.021579  0.11\n",
       "Carmel         0.000  0.014762  0.16\n",
       "Los Angeles    0.000  0.014091  0.22\n",
       "New York City  0.000  0.032000  0.23\n",
       "Orlando        0.000  0.048182  0.41\n",
       "Chicago        0.000  0.050476  0.41\n",
       "Vancouver      0.020  0.084737  0.46\n",
       "Seattle        0.015  0.077813  0.47\n",
       "Minneapolis    0.000  0.047143  0.49\n",
       "Houston        0.000  0.072273  0.52\n",
       "Portland       0.010  0.094762  0.66\n",
       "Toronto        0.015  0.135000  0.69\n",
       "Sacramento     0.005  0.122500  0.76\n",
       "Philadelphia   0.000  0.067143  0.86\n",
       "Montreal       0.000  0.082500  0.89\n",
       "Washington     0.005  0.180000  0.90\n",
       "Duluth         0.020  0.177143  1.03\n",
       "Honololu       0.000  0.139048  2.00\n",
       "Boston         0.000  0.147273  2.24"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the weather data grouped by city if interested. Could look at PRCP, TEMP, MIN, MAX, etc.\n",
    "weather_df.groupby(by=['City']).agg({\"PRCP\":[np.median,np.mean,np.max]}).sort_values(('PRCP','amax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3729e140-1884-4e21-8f87-ef5339e1d5d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 5. Inner Join Race results, elevation, and weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f66c261-3c7d-4e7d-8619-f94beb9aa486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load marathon results\n",
    "directory = 'results_csvs'\n",
    "newest_results = [file for file in sorted(os.listdir(directory)) if file.endswith('.csv')][-1]\n",
    "results_df = pd.read_csv(os.path.join(directory, newest_results))\n",
    "results_df['Date'] = pd.to_datetime(results_df['Date'])\n",
    "time_cols = ['Male Win','Female Win','Average Time','Time STD']\n",
    "results_df[time_cols] = (results_df[time_cols].apply(pd.to_timedelta))\n",
    "# Add column of sex ratio\n",
    "results_df['Percent Female'] = 100*results_df['Females']/(results_df['Females']+results_df['Males'])\n",
    "# Drop Boston 2013 because course was close mid-race for emergency\n",
    "Bos_2013 = results_df[(results_df['Event Name'] == 'Boston Marathon') & (results_df['Date'].dt.year == 2013)].index\n",
    "results_df = results_df.drop(Bos_2013)\n",
    "\n",
    "# Load elevation data\n",
    "directory = 'elevation_csvs'\n",
    "newest_elevation = [file for file in sorted(os.listdir(directory)) if file.endswith('.csv')][-1]\n",
    "elevation_df = pd.read_csv(os.path.join(directory, newest_elevation))\n",
    "\n",
    "# Load weather data\n",
    "directory = 'weather_table_csvs'\n",
    "newest_weather = [file for file in sorted(os.listdir(directory)) if file.endswith('.csv')][-1]\n",
    "weather_df = pd.read_csv(os.path.join(directory, newest_weather))\n",
    "weather_df['Date'] = pd.to_datetime(weather_df['Date'])\n",
    "\n",
    "# Join all three data sets\n",
    "full_df = (results_df\n",
    "           .merge(elevation_df, how='inner', on='Event Name')\n",
    "           .merge(weather_df, how='inner', on=['Date','City','State'])\n",
    "           .sort_values(['Event Name','Date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cdc2788-ca06-46d6-99db-36a9e88339f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Event ID</th>\n",
       "      <th>Event Name</th>\n",
       "      <th>Date</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Finishers</th>\n",
       "      <th>Males</th>\n",
       "      <th>Females</th>\n",
       "      <th>Male Win</th>\n",
       "      <th>Female Win</th>\n",
       "      <th>...</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>MAX</th>\n",
       "      <th>MIN</th>\n",
       "      <th>PRCP</th>\n",
       "      <th>DEWP</th>\n",
       "      <th>WDSP</th>\n",
       "      <th>SLP</th>\n",
       "      <th>STP</th>\n",
       "      <th>VISIB</th>\n",
       "      <th>RELHUM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>18000430</td>\n",
       "      <td>Big Sur International Marathon</td>\n",
       "      <td>2000-04-30</td>\n",
       "      <td>Carmel</td>\n",
       "      <td>CA</td>\n",
       "      <td>2407</td>\n",
       "      <td>1535</td>\n",
       "      <td>872</td>\n",
       "      <td>0 days 02:27:06</td>\n",
       "      <td>0 days 02:46:53</td>\n",
       "      <td>...</td>\n",
       "      <td>53.6</td>\n",
       "      <td>63.0</td>\n",
       "      <td>42.1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>45.7</td>\n",
       "      <td>5.4</td>\n",
       "      <td>1022.0</td>\n",
       "      <td>999.9</td>\n",
       "      <td>10.0</td>\n",
       "      <td>74.520777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>18010429</td>\n",
       "      <td>Big Sur International Marathon</td>\n",
       "      <td>2001-04-29</td>\n",
       "      <td>Carmel</td>\n",
       "      <td>CA</td>\n",
       "      <td>2560</td>\n",
       "      <td>1548</td>\n",
       "      <td>1012</td>\n",
       "      <td>0 days 02:25:38</td>\n",
       "      <td>0 days 02:46:41</td>\n",
       "      <td>...</td>\n",
       "      <td>52.8</td>\n",
       "      <td>60.8</td>\n",
       "      <td>46.4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>44.8</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1021.2</td>\n",
       "      <td>999.9</td>\n",
       "      <td>10.0</td>\n",
       "      <td>74.161050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>18020428</td>\n",
       "      <td>Big Sur International Marathon</td>\n",
       "      <td>2002-04-28</td>\n",
       "      <td>Carmel</td>\n",
       "      <td>CA</td>\n",
       "      <td>2343</td>\n",
       "      <td>1492</td>\n",
       "      <td>851</td>\n",
       "      <td>0 days 02:18:05</td>\n",
       "      <td>0 days 02:51:10</td>\n",
       "      <td>...</td>\n",
       "      <td>49.8</td>\n",
       "      <td>60.1</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>41.1</td>\n",
       "      <td>5.9</td>\n",
       "      <td>1021.1</td>\n",
       "      <td>999.9</td>\n",
       "      <td>10.0</td>\n",
       "      <td>71.896619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>18030427</td>\n",
       "      <td>Big Sur International Marathon</td>\n",
       "      <td>2003-04-27</td>\n",
       "      <td>Carmel</td>\n",
       "      <td>CA</td>\n",
       "      <td>2820</td>\n",
       "      <td>1755</td>\n",
       "      <td>1065</td>\n",
       "      <td>0 days 02:19:59</td>\n",
       "      <td>0 days 02:47:11</td>\n",
       "      <td>...</td>\n",
       "      <td>54.2</td>\n",
       "      <td>63.0</td>\n",
       "      <td>44.1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.1</td>\n",
       "      <td>7.1</td>\n",
       "      <td>1016.9</td>\n",
       "      <td>999.9</td>\n",
       "      <td>10.0</td>\n",
       "      <td>61.137048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>18040425</td>\n",
       "      <td>Big Sur International Marathon</td>\n",
       "      <td>2004-04-25</td>\n",
       "      <td>Carmel</td>\n",
       "      <td>CA</td>\n",
       "      <td>2858</td>\n",
       "      <td>1815</td>\n",
       "      <td>1043</td>\n",
       "      <td>0 days 02:26:19</td>\n",
       "      <td>0 days 03:10:06</td>\n",
       "      <td>...</td>\n",
       "      <td>62.5</td>\n",
       "      <td>80.6</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1016.7</td>\n",
       "      <td>999.9</td>\n",
       "      <td>10.0</td>\n",
       "      <td>59.046912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>58161009</td>\n",
       "      <td>Twin Cities Marathon</td>\n",
       "      <td>2016-10-09</td>\n",
       "      <td>Minneapolis</td>\n",
       "      <td>MN</td>\n",
       "      <td>8551</td>\n",
       "      <td>4714</td>\n",
       "      <td>3837</td>\n",
       "      <td>0 days 02:08:51</td>\n",
       "      <td>0 days 02:30:01</td>\n",
       "      <td>...</td>\n",
       "      <td>46.0</td>\n",
       "      <td>60.1</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>35.9</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1027.8</td>\n",
       "      <td>996.7</td>\n",
       "      <td>8.9</td>\n",
       "      <td>67.650899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>58171001</td>\n",
       "      <td>Twin Cities Marathon</td>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>Minneapolis</td>\n",
       "      <td>MN</td>\n",
       "      <td>7518</td>\n",
       "      <td>4100</td>\n",
       "      <td>3418</td>\n",
       "      <td>0 days 02:11:53</td>\n",
       "      <td>0 days 02:30:25</td>\n",
       "      <td>...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>69.1</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>48.6</td>\n",
       "      <td>11.1</td>\n",
       "      <td>1017.2</td>\n",
       "      <td>986.4</td>\n",
       "      <td>9.5</td>\n",
       "      <td>65.976075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>58181007</td>\n",
       "      <td>Twin Cities Marathon</td>\n",
       "      <td>2018-10-07</td>\n",
       "      <td>Minneapolis</td>\n",
       "      <td>MN</td>\n",
       "      <td>7157</td>\n",
       "      <td>3972</td>\n",
       "      <td>3185</td>\n",
       "      <td>0 days 02:11:58</td>\n",
       "      <td>0 days 02:33:04</td>\n",
       "      <td>...</td>\n",
       "      <td>47.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>44.1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.7</td>\n",
       "      <td>7.1</td>\n",
       "      <td>1024.9</td>\n",
       "      <td>993.4</td>\n",
       "      <td>9.1</td>\n",
       "      <td>81.711865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>58191006</td>\n",
       "      <td>Twin Cities Marathon</td>\n",
       "      <td>2019-10-06</td>\n",
       "      <td>Minneapolis</td>\n",
       "      <td>MN</td>\n",
       "      <td>6739</td>\n",
       "      <td>3849</td>\n",
       "      <td>2890</td>\n",
       "      <td>0 days 02:12:23</td>\n",
       "      <td>0 days 02:31:29</td>\n",
       "      <td>...</td>\n",
       "      <td>54.0</td>\n",
       "      <td>64.9</td>\n",
       "      <td>46.9</td>\n",
       "      <td>0.49</td>\n",
       "      <td>41.9</td>\n",
       "      <td>11.7</td>\n",
       "      <td>1012.6</td>\n",
       "      <td>982.0</td>\n",
       "      <td>9.9</td>\n",
       "      <td>63.516169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>58211003</td>\n",
       "      <td>Twin Cities Marathon</td>\n",
       "      <td>2021-10-03</td>\n",
       "      <td>Minneapolis</td>\n",
       "      <td>MN</td>\n",
       "      <td>3203</td>\n",
       "      <td>1868</td>\n",
       "      <td>1335</td>\n",
       "      <td>0 days 02:15:22</td>\n",
       "      <td>0 days 02:45:55</td>\n",
       "      <td>...</td>\n",
       "      <td>64.7</td>\n",
       "      <td>73.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>57.9</td>\n",
       "      <td>7.7</td>\n",
       "      <td>1012.9</td>\n",
       "      <td>982.8</td>\n",
       "      <td>9.6</td>\n",
       "      <td>78.610980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>406 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Event ID                      Event Name       Date         City State  \\\n",
       "384  18000430  Big Sur International Marathon 2000-04-30       Carmel    CA   \n",
       "383  18010429  Big Sur International Marathon 2001-04-29       Carmel    CA   \n",
       "382  18020428  Big Sur International Marathon 2002-04-28       Carmel    CA   \n",
       "381  18030427  Big Sur International Marathon 2003-04-27       Carmel    CA   \n",
       "380  18040425  Big Sur International Marathon 2004-04-25       Carmel    CA   \n",
       "..        ...                             ...        ...          ...   ...   \n",
       "240  58161009            Twin Cities Marathon 2016-10-09  Minneapolis    MN   \n",
       "239  58171001            Twin Cities Marathon 2017-10-01  Minneapolis    MN   \n",
       "238  58181007            Twin Cities Marathon 2018-10-07  Minneapolis    MN   \n",
       "237  58191006            Twin Cities Marathon 2019-10-06  Minneapolis    MN   \n",
       "236  58211003            Twin Cities Marathon 2021-10-03  Minneapolis    MN   \n",
       "\n",
       "     Finishers  Males  Females        Male Win      Female Win  ...  TEMP  \\\n",
       "384       2407   1535      872 0 days 02:27:06 0 days 02:46:53  ...  53.6   \n",
       "383       2560   1548     1012 0 days 02:25:38 0 days 02:46:41  ...  52.8   \n",
       "382       2343   1492      851 0 days 02:18:05 0 days 02:51:10  ...  49.8   \n",
       "381       2820   1755     1065 0 days 02:19:59 0 days 02:47:11  ...  54.2   \n",
       "380       2858   1815     1043 0 days 02:26:19 0 days 03:10:06  ...  62.5   \n",
       "..         ...    ...      ...             ...             ...  ...   ...   \n",
       "240       8551   4714     3837 0 days 02:08:51 0 days 02:30:01  ...  46.0   \n",
       "239       7518   4100     3418 0 days 02:11:53 0 days 02:30:25  ...  60.0   \n",
       "238       7157   3972     3185 0 days 02:11:58 0 days 02:33:04  ...  47.0   \n",
       "237       6739   3849     2890 0 days 02:12:23 0 days 02:31:29  ...  54.0   \n",
       "236       3203   1868     1335 0 days 02:15:22 0 days 02:45:55  ...  64.7   \n",
       "\n",
       "      MAX   MIN  PRCP  DEWP  WDSP     SLP    STP  VISIB     RELHUM  \n",
       "384  63.0  42.1  0.00  45.7   5.4  1022.0  999.9   10.0  74.520777  \n",
       "383  60.8  46.4  0.00  44.8   5.8  1021.2  999.9   10.0  74.161050  \n",
       "382  60.1  41.0  0.08  41.1   5.9  1021.1  999.9   10.0  71.896619  \n",
       "381  63.0  44.1  0.00  41.1   7.1  1016.9  999.9   10.0  61.137048  \n",
       "380  80.6  50.0  0.00  48.0   2.3  1016.7  999.9   10.0  59.046912  \n",
       "..    ...   ...   ...   ...   ...     ...    ...    ...        ...  \n",
       "240  60.1  36.0  0.00  35.9   3.9  1027.8  996.7    8.9  67.650899  \n",
       "239  69.1  46.0  0.06  48.6  11.1  1017.2  986.4    9.5  65.976075  \n",
       "238  52.0  44.1  0.00  41.7   7.1  1024.9  993.4    9.1  81.711865  \n",
       "237  64.9  46.9  0.49  41.9  11.7  1012.6  982.0    9.9  63.516169  \n",
       "236  73.0  57.0  0.26  57.9   7.7  1012.9  982.8    9.6  78.610980  \n",
       "\n",
       "[406 rows x 31 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Output\n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "732a3f02-e998-48e0-b2da-d5b84c499e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a copy of the full data set\n",
    "directory = 'full_df_csvs'\n",
    "if not os.path.exists(directory):\n",
    "    os.mkdir(directory)\n",
    "    print(f'Directory created: {directory}')\n",
    "\n",
    "now = datetime.today().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "full_df.to_csv(f'{directory}/full_{now}.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci5",
   "language": "python",
   "name": "datasci5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
